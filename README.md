# ML-Learning-Resources
Consolidated ML Resources


## RNN
* [Introduction to RNN and LSTM](https://deeplearning4j.org/lstm.html)
* [A good video introduction by Siraj Raval](https://www.youtube.com/watch?v=BwmddtPFWtA)
* [Recent Advances in RNN](https://arxiv.org/pdf/1801.01078.pdf)
* [Sequence Transduction with Recurrent Neural Networks](https://arxiv.org/pdf/1211.3711.pdf)
* [Learning Phrase Representations using RNN Encoder–Decoder](https://arxiv.org/pdf/1406.1078.pdf)
* [Deep RL for sequence to sequence models](https://arxiv.org/pdf/1805.09461.pdf)

## Attention Networks
* [Transformer - Attention model explained](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.Wy3sXRIza34)
* [Google Brain Attention paper](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)
* [Syntactic constituency parsing using Attention models](https://arxiv.org/pdf/1412.7449.pdf)

## Natural Language Processing

- [A curated list of speech and natural language processing resources](https://github.com/edobashira/speech-language-processing)

- [tf-idf explained](http://michaelerasm.us/post/tf-idf-in-10-minutes/)

- [Interesting Deep Learning NLP Projects Stanford](http://cs224d.stanford.edu/reports.html), [Website](http://cs224d.stanford.edu/)

- [NLP from Scratch | Google Paper](https://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/35671.pdf)

- [Graph Based Semi Supervised Learning for NLP](http://graph-ssl.wdfiles.com/local--files/blog%3A_start/graph_ssl_acl12_tutorial_slides_final.pdf)

- [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model)

    - [Classification text with Bag of Words](http://fastml.com/classifying-text-with-bag-of-words-a-tutorial/)
    
- [Topic Modeling](https://en.wikipedia.org/wiki/Topic_model)

    - [LDA](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation), [LSA](https://en.wikipedia.org/wiki/Latent_semantic_analysis), [Probabilistic LSA](https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis)
    
    - [What is a good explanation of Latent Dirichlet Allocation?](https://www.quora.com/What-is-a-good-explanation-of-Latent-Dirichlet-Allocation)
    
    - [Awesome LDA Explanation!](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/). [Another good explanation](http://confusedlanguagetech.blogspot.in/2012/07/jordan-boyd-graber-and-philip-resnik.html)
    
    - [The LDA Buffet- Intuitive Explanation](http://www.matthewjockers.net/2011/09/29/the-lda-buffet-is-now-open-or-latent-dirichlet-allocation-for-english-majors/)
    
    - [Difference between LSI and LDA](https://www.quora.com/Whats-the-difference-between-Latent-Semantic-Indexing-LSI-and-Latent-Dirichlet-Allocation-LDA)
    
    - [Original LDA Paper](https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf)
    
    - [alpha and beta in LDA](http://datascience.stackexchange.com/questions/199/what-does-the-alpha-and-beta-hyperparameters-contribute-to-in-latent-dirichlet-a)
    
    - [Intuitive explanation of the Dirichlet distribution](https://www.quora.com/What-is-an-intuitive-explanation-of-the-Dirichlet-distribution)
    
    - [Topic modeling made just simple enough](https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/)
    
    - [Online LDA](http://alexminnaar.com/online-latent-dirichlet-allocation-the-best-option-for-topic-modeling-with-large-data-sets.html), [Online LDA with Spark](http://alexminnaar.com/distributed-online-latent-dirichlet-allocation-with-apache-spark.html)
    
    - [LDA in Scala](http://alexminnaar.com/latent-dirichlet-allocation-in-scala-part-i-the-theory.html), [Part 2](http://alexminnaar.com/latent-dirichlet-allocation-in-scala-part-ii-the-code.html)
    
    - [Segmentation of Twitter Timelines via Topic Modeling](http://alexperrier.github.io/jekyll/update/2015/09/16/segmentation_twitter_timelines_lda_vs_lsa.html)
    
    - [Topic Modeling of Twitter Followers](http://alexperrier.github.io/jekyll/update/2015/09/04/topic-modeling-of-twitter-followers.html)

<a name="word2vec" />

- word2vec

    - [Google word2vec](https://code.google.com/archive/p/word2vec)
    
    - [Bag of Words Model Wiki](https://en.wikipedia.org/wiki/Bag-of-words_model)
    
    - [word2vec Tutorial](https://rare-technologies.com/word2vec-tutorial/)
    
    - [A closer look at Skip Gram Modeling](http://homepages.inf.ed.ac.uk/ballison/pdf/lrec_skipgrams.pdf)
    
    - [Skip Gram Model Tutorial](http://alexminnaar.com/word2vec-tutorial-part-i-the-skip-gram-model.html), [CBoW Model](http://alexminnaar.com/word2vec-tutorial-part-ii-the-continuous-bag-of-words-model.html)
    
    - [Word Vectors Kaggle Tutorial Python](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors), [Part 2](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors)
    
    - [Making sense of word2vec](http://rare-technologies.com/making-sense-of-word2vec/)
    
    - [word2vec explained on deeplearning4j](http://deeplearning4j.org/word2vec.html)
    
    - [Quora word2vec](https://www.quora.com/How-does-word2vec-work)
    
    - [Other Quora Resources](https://www.quora.com/What-are-the-continuous-bag-of-words-and-skip-gram-architectures-in-laymans-terms), [2](https://www.quora.com/What-is-the-difference-between-the-Bag-of-Words-model-and-the-Continuous-Bag-of-Words-model), [3](https://www.quora.com/Is-skip-gram-negative-sampling-better-than-CBOW-NS-for-word2vec-If-so-why)
    
    - [word2vec, DBN, RNTN for Sentiment Analysis ](http://deeplearning4j.org/zh-sentiment_analysis_word2vec.html)

- Text Clustering

    - [How string clustering works](http://stackoverflow.com/questions/8196371/how-clustering-works-especially-string-clustering)
    
    - [Levenshtein distance for measuring the difference between two sequences](https://en.wikipedia.org/wiki/Levenshtein_distance)
    
    - [Text clustering with Levenshtein distances](http://stackoverflow.com/questions/21511801/text-clustering-with-levenshtein-distances)

- Text Classification

    - [Classification Text with Bag of Words](http://fastml.com/classifying-text-with-bag-of-words-a-tutorial/)

- [Language learning with NLP and reinforcement learning](http://blog.dennybritz.com/2015/09/11/reimagining-language-learning-with-nlp-and-reinforcement-learning/)

- [Kaggle Tutorial Bag of Words and Word vectors](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words), [Part 2](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors), [Part 3](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors)

- [What would Shakespeare say (NLP Tutorial)](https://gigadom.wordpress.com/2015/10/02/natural-language-processing-what-would-shakespeare-say/)

- [A closer look at Skip Gram Modeling](http://homepages.inf.ed.ac.uk/ballison/pdf/lrec_skipgrams.pdf)

## Healthcare & Machine Learning

* [Machine Learning for Healthcare](https://mlhc17mit.github.io/) - MIT course 2017. Lecture notes only.


## Deep Learning Courses

* [Berkeley] [CS294: Deep Reinforcement Learning](http://rll.berkeley.edu/deeprlcourse/)
* [Berkeley] [Stat212b：Topics Course on Deep Learning](http://joanbruna.github.io/stat212b/)
* [CUHK] [ELEG 5040: Advanced Topics in Signal Processing(Introduction to Deep Learning)](https://piazza.com/cuhk.edu.hk/spring2015/eleg5040/home)
* [CMU] [Deep Reinforcement Learning and Control](https://katefvision.github.io/)
* [CMU] [Introduction to Deep Learning](http://deeplearning.cs.cmu.edu/)
* [CMU] [Neural networks for NLP](http://phontron.com/class/nn4nlp2018/)
* [COMS] [W4995 Applied Machine Learning Spring 2018](http://www.cs.columbia.edu/~amueller/comsw4995s18/)
* [David Silver] [RL Course](https://www.youtube.com/watch?v=2pWv7GOvuf0&index=1&list=PL5X3mDkKaJrL42i_jhE4N-p6E2Ol62Ofa)
* [EE 227C][Convex Optimization and Approximation](https://ee227c.github.io/)
* [Emory University] [CS584: Deep Learning](http://nematilab.info/CS584.html)
* [Google] [Udacity Deep Learning Online Course](https://www.youtube.com/watch?v=X_B9NADf2wk&list=PLAwxTw4SYaPn_OWPFT9ulXLuQrImzHfOV&index=2)
* [Hinton] [Neural Networks for Machine Learning](https://www.coursera.org/learn/neural-networks)
* [Hvass Laboratories] [TensorFlow](https://www.youtube.com/playlist?list=PL9Hr9sNUjfsmEu1ZniY0XpHSzl5uihcXZ)
* [INTEL] [Deep Learning 501](https://software.intel.com/en-us/ai-academy/students/kits/deep-learning-501)
* [Jeremy Howard] [Deep Learning For Coders](https://www.youtube.com/playlist?list=PLfYUBJiXbdtS2UQRzyrxmyVHoGW0gmLSM)
* [MIT] [Practical Deep LeTensorFlowarning For Coders](http://course.fast.ai/index.html)
* [MIT] [S099: Artificial General Intelligence](https://agi.mit.edu/)
* [MIT] [S094: Deep Learning for Self-Driving Cars](https://selfdrivingcars.mit.edu/)
* [MIT] [S191: Introduction to Deep Learning](http://introtodeeplearning.com/)
* [Nvidia] [Fundamentals of Accelerated Computing with CUDA C/C++](https://courses.nvidia.com/courses/course-v1:DLI+C-AC-01+V1/about)
* [NYU] [Deep Learning by Prof. Yann LeCun](http://cilvr.cs.nyu.edu/doku.php?id=courses:deeplearning2014:start)
* [Oxford] [Deep Learning Course](http://www.computervisiontalks.com/tag/deep-learning-course/)
* [Oxford] [Deep Learning by Prof. Nando de Freitas](https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/)
* [Paris Saclay][Deep Learning course: lecture slides and lab notebooks](https://m2dsupsdlclass.github.io/lectures-labs/)
* [Stanford] [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)
* [Stanford] [CS20SI: Tensorflow for Deep Learning Research](https://web.stanford.edu/class/cs20si/)
* [Stanford] [CS224n: Natural Language Processing with Deep Learning](https://web.stanford.edu/class/cs224n/index.html)
* [Stanford] [CS 228: Probabilistic Graphical Models](http://cs.stanford.edu/~ermon/cs228/index.html)
* [Stanford] [CS 20: Tensorflow for Deep Learning Research](https://web.stanford.edu/class/cs20si/)
* [Stanford] [STATS 385: Theories of Deep Learning](https://stats385.github.io/)
* [Toronto] [CSC 2541 Fall 2016:Differentiable Inference and Generative Models](http://www.cs.toronto.edu/~duvenaud/courses/csc2541/index.html)
* [Utah] [Applied Computational Genomics Course at UU](https://github.com/quinlan-lab/applied-computational-genomics)
* [YouTube][OpenCV with Python for Image and Video Analysis](https://www.youtube.com/playlist?list=PLQVvvaa0QuDdttJXlLtAJxJetJcqmqlQq)
* [Python for Data Analysis](https://github.com/cuttlefishh/python-for-data-analysis)
* [Statistical and Discrete Methods for Scientific Computing](http://wpressutexas.net/coursewiki/index.php?title=Main_Page)
* [ANDREW NG][deeplearning.ai](https://www.deeplearning.ai/)
* [Introductory Machine Learning Algorithms in Python with scikit-learn](https://egghead.io/courses/introductory-machine-learning-algorithms-in-python-with-scikit-learn)
* [YouTube] [Deep Learning Crash Course (2018)](https://www.youtube.com/playlist?list=PLWKotBjTDoLj3rXBL-nEIPRN9V3a9Cx07)

## Interview Resources

* [41 Essential Machine Learning Interview Questions (with answers)](https://www.springboard.com/blog/machine-learning-interview-questions/)
* [How can a computer science graduate student prepare himself for data scientist interviews?](https://www.quora.com/How-can-a-computer-science-graduate-student-prepare-himself-for-data-scientist-machine-learning-intern-interviews)
* [How do I learn Machine Learning?](https://www.quora.com/How-do-I-learn-machine-learning-1)
* [FAQs about Data Science Interviews](https://www.quora.com/topic/Data-Science-Interviews/faq)
* [What are the key skills of a data scientist?](https://www.quora.com/What-are-the-key-skills-of-a-data-scientist)
* [The Big List of DS/ML Interview Resources](https://towardsdatascience.com/the-big-list-of-ds-ml-interview-resources-2db4f651bd63)

## Acknowledgements
* MIT Open Course Ware
* Stanford 
* MIT
* CMU
* NYU
* Coursera
* Google
* Oxford
* Nvidia
* UofToronto
* Berkeley
* CUHK
* Emory University
* @ujjwalkarn

